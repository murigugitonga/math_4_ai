{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3a8OPr6QTi3HgeWi29cRL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/murigugitonga/math_4_ai/blob/dev/03_calculus/08_jacobian_and_hessian.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculus — Jacobian & Hessian\n",
        "\n",
        "**Author:** Murigu Gitonga  \n",
        "**Objective:** Demonstrate Jacobian and Hessian matrices and their role in\n",
        "multivariate optimization, neural networks & second-order learning methods.\n"
      ],
      "metadata": {
        "id": "KQbyQTDtC70f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Why Do We Need Them?\n",
        "\n",
        "- Gradients work for **scalar functions**\n",
        "- ML models often involve:\n",
        "  - Vector-valued functions\n",
        "  - Many parameters\n",
        "- Jacobian -> first-order behavior of vector functions\n",
        "- Hessian -> second-order curvature information\n",
        "\n",
        "These concepts explain:\n",
        "- Backpropagation mechanics\n",
        "- Vanishing / exploding gradients\n",
        "- Why optimization can be slow or unstable\n"
      ],
      "metadata": {
        "id": "B-9vA-BpDCM8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Jacobian Matrix\n",
        "\n",
        "Let:\n",
        "$$\n",
        "\\mathbf{f}(\\mathbf{x}) =\n",
        "\\begin{bmatrix}\n",
        "f_1(x_1, x_2, \\dots, x_n) \\\\\n",
        "f_2(x_1, x_2, \\dots, x_n) \\\\\n",
        "\\vdots \\\\\n",
        "f_m(x_1, x_2, \\dots, x_n)\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The Jacobian matrix is:\n",
        "$$\n",
        "J =\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
        "\\frac{\\partial f_2}{\\partial x_1} & \\cdots & \\frac{\\partial f_2}{\\partial x_n} \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n",
        "\\end{bmatrix}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "P9lrXLivDGkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example\n",
        "\n",
        "Let:\n",
        "$$\n",
        "\\mathbf{f}(x,y) =\n",
        "\\begin{bmatrix}\n",
        "x^2 + y \\\\\n",
        "\\sin(xy)\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Partial derivatives:\n",
        "$$\n",
        "\\frac{\\partial f_1}{\\partial x} = 2x, \\quad\n",
        "\\frac{\\partial f_1}{\\partial y} = 1\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial f_2}{\\partial x} = y\\cos(xy), \\quad\n",
        "\\frac{\\partial f_2}{\\partial y} = x\\cos(xy)\n",
        "$$\n"
      ],
      "metadata": {
        "id": "kqpZQDCgDRp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def jacobian(x, y):\n",
        "  return np.array([[2*x, 1], [y*np.cos(x*y), x*np.cos(x*y)]])\n",
        "\n",
        "jacobian(1.0, 2.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnpK8k-IDVam",
        "outputId": "678aef04-4434-4e1b-e5b2-e4b7abb59a55"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.        ,  1.        ],\n",
              "       [-0.83229367, -0.41614684]])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Jacobian in Machine Learning\n",
        "\n",
        "- Jacobian appears in:\n",
        "  - Backpropagation\n",
        "  - Change of variables\n",
        "  - Sensitivity analysis\n",
        "- Each layer’s output depends on previous layer -> Jacobian chain\n",
        "- Backprop = multiplying Jacobians efficiently\n"
      ],
      "metadata": {
        "id": "-1r4cjqLD8h4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Hessian Matrix\n",
        "\n",
        "For scalar function:\n",
        "$$\n",
        "f(x_1, x_2, \\dots, x_n)\n",
        "$$\n",
        "\n",
        "Hessian matrix:\n",
        "$$\n",
        "H =\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial^2 f}{\\partial x_1^2} &\n",
        "\\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots \\\\\n",
        "\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &\n",
        "\\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots \\\\\n",
        "\\vdots & \\vdots & \\ddots\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "- Hessian captures **curvature**\n",
        "- Used in second-order optimization\n"
      ],
      "metadata": {
        "id": "KqQ-Ji9iEBI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example\n",
        "\n",
        "Let:\n",
        "$$\n",
        "f(x,y) = x^2 + y^2\n",
        "$$\n",
        "\n",
        "Second derivatives:\n",
        "$$\n",
        "\\frac{\\partial^2 f}{\\partial x^2} = 2, \\quad\n",
        "\\frac{\\partial^2 f}{\\partial y^2} = 2\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial^2 f}{\\partial x \\partial y} = 0\n",
        "$$\n",
        "\n",
        "Hessian:\n",
        "$$\n",
        "H =\n",
        "\\begin{bmatrix}\n",
        "2 & 0 \\\\\n",
        "0 & 2\n",
        "\\end{bmatrix}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "iQssxyn3EJV6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX7LR7RSCtMS",
        "outputId": "bafdf830-aef3-4808-8cc2-c91ee2e8c54b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2, 0],\n",
              "       [0, 2]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Hessian in Python\n",
        "\n",
        "def hessian():\n",
        "  return np.array([\n",
        "      [2, 0],\n",
        "      [0, 2]\n",
        "  ])\n",
        "\n",
        "hessian()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Why Curvature Matters\n",
        "\n",
        "- Flat regions -> slow learning\n",
        "- Sharp curvature -> unstable updates\n",
        "- Hessian eigenvalues indicate:\n",
        "  - Positive definite -> minimum\n",
        "  - Negative definite -> maximum\n",
        "  - Mixed -> saddle point\n"
      ],
      "metadata": {
        "id": "9A55wsPPEiUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Newton’s Method\n",
        "\n",
        "Update rule:\n",
        "$$\n",
        "\\mathbf{w}_{t+1} =\n",
        "\\mathbf{w}_t - H^{-1} \\nabla f(\\mathbf{w}_t)\n",
        "$$\n",
        "\n",
        "- Uses curvature information\n",
        "- Faster convergence\n",
        "- Expensive for large models\n"
      ],
      "metadata": {
        "id": "KCJo8qoeEmw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Newton's Method in 1D\n",
        "\n",
        "def f(w):\n",
        "  return (w -3) ** 2\n",
        "\n",
        "def grad(w):\n",
        "  return 2 * (w - 3)\n",
        "\n",
        "def hess(w):\n",
        "  return 2\n",
        "\n",
        "w = 0.0\n",
        "\n",
        "for _ in range(5):\n",
        "  w = w - grad(w)/ hess(w)\n",
        "\n",
        "w\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be1qKYP9EuyZ",
        "outputId": "fee0dd04-fac4-45a0-9667-b2a48f31c395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.0"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Key Takeaways\n",
        "\n",
        "- Jacobian = first-order behavior of vector functions\n",
        "- Hessian = second-order curvature\n",
        "- Backprop uses Jacobians\n",
        "- Second-order methods use Hessians\n",
        "- Curvature explains optimization difficulty\n"
      ],
      "metadata": {
        "id": "fm-ERtAwFUVR"
      }
    }
  ]
}